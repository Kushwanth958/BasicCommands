{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Books to Scrape — full dataset scraper (≥500 rows)\n",
        "Collects title, price_gbp, availability_text, stock_count, rating_stars,\n",
        "category, upc, description, product_page_url. Saves to books_dataset.csv\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as BS\n",
        "from tqdm import tqdm\n",
        "\n",
        "BASE = \"https://books.toscrape.com/\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"StudentScraper/1.1 (+academic use; contact: student@example.com)\"\n",
        "}\n",
        "TIMEOUT = 20\n",
        "SLEEP_LOW, SLEEP_HIGH = 0.75, 1.25  # polite throttle\n",
        "\n",
        "\n",
        "def polite_sleep(a=SLEEP_LOW, b=SLEEP_HIGH):\n",
        "    time.sleep(random.uniform(a, b))\n",
        "\n",
        "\n",
        "def get_soup(url, retries=3):\n",
        "    \"\"\"GET a URL with simple retry and return BeautifulSoup.\"\"\"\n",
        "    last_err = None\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "            return BS(resp.text, \"html.parser\")\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            polite_sleep(1.0, 2.0)\n",
        "    raise last_err\n",
        "\n",
        "\n",
        "def extract_rating(soup):\n",
        "    \"\"\"Map CSS class to 1–5 integer stars.\"\"\"\n",
        "    rating_p = soup.select_one(\"p.star-rating\")\n",
        "    if not rating_p:\n",
        "        return None\n",
        "    classes = rating_p.get(\"class\", [])\n",
        "    mapping = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
        "    for c in classes:\n",
        "        if c in mapping:\n",
        "            return mapping[c]\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_stock(avail_text):\n",
        "    \"\"\"Extract first integer from availability text.\"\"\"\n",
        "    if not avail_text:\n",
        "        return None\n",
        "    m = re.search(r\"(\\d+)\", avail_text)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "\n",
        "def clean_price(raw_text):\n",
        "    \"\"\"\n",
        "    Robustly convert a price string like '£45.17' (possibly with NBSPs/spaces)\n",
        "    to a float 45.17.\n",
        "    \"\"\"\n",
        "    if raw_text is None:\n",
        "        return None\n",
        "    t = raw_text.replace(\"£\", \"\").replace(\"\\xa0\", \" \").strip()\n",
        "    # keep digits, dot, comma; remove any stray characters\n",
        "    t = re.sub(r\"[^0-9\\.,]\", \"\", t)\n",
        "    t = t.replace(\",\", \"\")  # ensure standard decimal\n",
        "    return float(t) if t else None\n",
        "\n",
        "\n",
        "def parse_product_page(url):\n",
        "    \"\"\"Parse a single product page and return a dict of fields.\"\"\"\n",
        "    soup = get_soup(url)\n",
        "    # Title\n",
        "    title_el = soup.select_one(\"div.product_main h1\")\n",
        "    title = title_el.get_text(strip=True) if title_el else None\n",
        "\n",
        "    # Price (FIXED: strip currency explicitly)\n",
        "    price_el = soup.select_one(\"p.price_color\")\n",
        "    price_gbp = clean_price(price_el.get_text()) if price_el else None\n",
        "\n",
        "    # Availability (and stock_count)\n",
        "    avail_el = soup.select_one(\"p.instock.availability\")\n",
        "    availability_text = (\n",
        "        \" \".join(avail_el.get_text(separator=\" \", strip=True).split())\n",
        "        if avail_el\n",
        "        else None\n",
        "    )\n",
        "    stock_count = extract_stock(availability_text)\n",
        "\n",
        "    # Rating\n",
        "    rating_stars = extract_rating(soup)\n",
        "\n",
        "    # Category via breadcrumb\n",
        "    category = None\n",
        "    bc_nodes = soup.select(\"ul.breadcrumb li\")\n",
        "    # typical: [Home] [Books] [Category] [Title]\n",
        "    if bc_nodes and len(bc_nodes) >= 3:\n",
        "        category = bc_nodes[2].get_text(strip=True)\n",
        "\n",
        "    # Metadata table (UPC lives here)\n",
        "    meta = {}\n",
        "    for row in soup.select(\"table.table.table-striped tr\"):\n",
        "        th = row.select_one(\"th\")\n",
        "        td = row.select_one(\"td\")\n",
        "        if th and td:\n",
        "            meta[th.get_text(strip=True)] = td.get_text(strip=True)\n",
        "    upc = meta.get(\"UPC\")\n",
        "\n",
        "    # Description (paragraph following the #product_description header)\n",
        "    desc_header = soup.select_one(\"#product_description\")\n",
        "    description = None\n",
        "    if desc_header:\n",
        "        p = desc_header.find_next(\"p\")\n",
        "        if p:\n",
        "            description = p.get_text(\" \", strip=True)\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"price_gbp\": price_gbp,\n",
        "        \"availability_text\": availability_text,\n",
        "        \"stock_count\": stock_count,\n",
        "        \"rating_stars\": rating_stars,\n",
        "        \"category\": category,\n",
        "        \"product_page_url\": url,\n",
        "        \"upc\": upc,\n",
        "        \"description\": description,\n",
        "    }\n",
        "\n",
        "\n",
        "def get_all_categories():\n",
        "    soup = get_soup(BASE)\n",
        "    polite_sleep()\n",
        "    cats = []\n",
        "    for a in soup.select(\"div.side_categories ul li ul li a\"):\n",
        "        href = a.get(\"href\")\n",
        "        name = a.get_text(strip=True)\n",
        "        url = urljoin(BASE, href)\n",
        "        cats.append((name, url))\n",
        "    return cats\n",
        "\n",
        "\n",
        "def category_pages(cat_url):\n",
        "    \"\"\"Yield all pagination URLs for a category (page-1, page-2, ...)\"\"\"\n",
        "    # walk next links until none remain\n",
        "    url = cat_url\n",
        "    while True:\n",
        "        yield url\n",
        "        soup = get_soup(url)\n",
        "        polite_sleep()\n",
        "        nxt = soup.select_one(\"li.next a\")\n",
        "        if not nxt:\n",
        "            break\n",
        "        url = urljoin(url, nxt.get(\"href\"))\n",
        "\n",
        "\n",
        "def list_product_links(listing_url):\n",
        "    \"\"\"Return product detail links found on a category listing page.\"\"\"\n",
        "    soup = get_soup(listing_url)\n",
        "    polite_sleep()\n",
        "    links = []\n",
        "    for a in soup.select(\"article.product_pod h3 a\"):\n",
        "        href = a.get(\"href\")\n",
        "        url = urljoin(listing_url, href)\n",
        "        links.append(url)\n",
        "    return links\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = []\n",
        "    categories = get_all_categories()\n",
        "    print(f\"Found {len(categories)} categories\")\n",
        "\n",
        "    for cat_name, cat_url in tqdm(categories, desc=\"Categories\"):\n",
        "        for page_url in category_pages(cat_url):\n",
        "            product_links = list_product_links(page_url)\n",
        "            for purl in product_links:\n",
        "                # normalize to absolute URL once more (handles ../../../)\n",
        "                purl = urljoin(page_url, purl)\n",
        "                try:\n",
        "                    rec = parse_product_page(purl)\n",
        "                    # fill category if breadcrumb missing\n",
        "                    if not rec.get(\"category\"):\n",
        "                        rec[\"category\"] = cat_name\n",
        "                    rows.append(rec)\n",
        "                except Exception as e:\n",
        "                    print(f\"[warn] {e} @ {purl}\", file=sys.stderr)\n",
        "                finally:\n",
        "                    polite_sleep()\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    # de-dup by UPC (unique)\n",
        "    if \"upc\" in df.columns:\n",
        "        df = df.drop_duplicates(subset=[\"upc\"]).reset_index(drop=True)\n",
        "\n",
        "    # ensure columns exist & order\n",
        "    cols = [\n",
        "        \"title\",\n",
        "        \"price_gbp\",\n",
        "        \"availability_text\",\n",
        "        \"stock_count\",\n",
        "        \"rating_stars\",\n",
        "        \"category\",\n",
        "        \"product_page_url\",\n",
        "        \"upc\",\n",
        "        \"description\",\n",
        "    ]\n",
        "    for c in cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = None\n",
        "    df = df[cols]\n",
        "\n",
        "    print(f\"Total rows scraped: {len(df)}\")\n",
        "    # sanity for assignment requirement\n",
        "    if len(df) < 500:\n",
        "        print(\n",
        "            \"Warning: fewer than 500 rows. Try re-running; site may have failed mid-run.\",\n",
        "            file=sys.stderr,\n",
        "        )\n",
        "\n",
        "    # save\n",
        "    out_csv = \"books_dataset.csv\"\n",
        "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Saved → {out_csv}\")\n",
        "\n",
        "    # optional parquet\n",
        "    try:\n",
        "        df.to_parquet(\"books_dataset.parquet\", index=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "aX-7UWcGDDoM",
        "outputId": "dee03a83-2291-45b8-b39d-fb7e83b4a916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 50 categories\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Categories:   8%|▊         | 4/50 [03:15<46:06, 60.13s/it]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l01c01_introduction_to_colab_and_python.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}